{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bs4\n",
    "#!pip install requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSTALLING THE LIBRARIES REQUIRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMORTING THE REQUIRED LIBRARIES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page=requests.get(\"https://en.wikipedia.org/wiki/Main_Page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESPONSE 200 MEANS THE DATA CAN BE FETCHED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACTING COMPLETE CONTENT FROM THE URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(page.content)\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO CONVERT THIS UNSTRUCTURED DATA TO STRUCTURED DATA I USED BEAUTIFUL SOUP FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO SEE THE STRUCTURED DATA WITH INDENTATION I USED PRETTIFY FUNCTION OVER SOUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a python program to display all the header tags from ‘en.wikipedia.org/wiki/Main_Page’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers=soup.find_all([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THERE ARE TOTAL 6 HEADERS IN THE HTML SO EXTRACTING THE HEADERS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_headers=[]\n",
    "for i in headers:\n",
    "    all_headers.append(i.text)\n",
    "all_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APPLIED FOR LOOP TO EXTRACT ALL THE HEADERS DATA FROM THE PAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_headers=[]\n",
    "for i in headers:\n",
    "    all_headers.append(i.text.replace(\"\\n\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALL THE HEADERS LIST WITH NO EXTRA DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headings(url):\n",
    "    p1 = requests.get(url)\n",
    "    \n",
    "    print(p1.status_code)\n",
    "    \n",
    "    s1 = BeautifulSoup(p1.content, 'html.parser')\n",
    "    \n",
    "    h1 = s1.find_all('span', class_=\"mw-headline\")\n",
    "    headings = []\n",
    "    for i in h1:\n",
    "        headings.append(i.get_text())\n",
    "        \n",
    "    print(headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headings(\"https://en.wikipedia.org/wiki/Main_Page\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER TO FIRST QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. Name, IMDB rating, Year of release) and save it in form of a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page2=requests.get(\"https://www.imdb.com/chart/top/?sort=rk,asc&mode=simple&page=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page2.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(page2.content)\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_names1=soup.find(\"td\",class_=\"titleColumn\")\n",
    "movie_names1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_names1.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_names=soup.find_all(\"td\",class_=\"titleColumn\")\n",
    "movie_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_names=[]\n",
    "for i in movie_names:\n",
    "    for j in i.find_all('a'):\n",
    "            m_names.append(j.text)\n",
    "print(m_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings= soup.find_all('td', class_='ratingColumn imdbRating')\n",
    "Movie_Rating = []\n",
    "for i in ratings:\n",
    "    for j in i.find_all('strong'):\n",
    "        Movie_Rating.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " year = soup.find_all('span', class_='secondaryInfo') \n",
    "Y_o_r = []\n",
    "\n",
    "for i in year:\n",
    "    Y_o_r.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.DataFrame({})\n",
    "data[\"names\"]=m_names[0:100]\n",
    "data[\"ratings\"]=Movie_Rating[0:100]\n",
    "data[\"yearofrelease\"]=Y_o_r[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of IMDB Movie Ratings is {} rows and {} columns \\n.\".format(*data.shape))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_Movie(url):\n",
    "    p2 = requests.get(url)\n",
    "    \n",
    "    print(p2.status_code)\n",
    "    \n",
    "    s2 = BeautifulSoup(p2.content, \"html.parser\")\n",
    "    \n",
    "    t1 = s2.find_all('td', class_='titleColumn') \n",
    "    Title = []\n",
    "    for i in t1:\n",
    "        for j in i.find_all('a'):\n",
    "            Title.append(j.text)\n",
    "            \n",
    "    y1 = s2.find_all('span', class_='secondaryInfo') \n",
    "    Release = []\n",
    "    for i in y1:\n",
    "        Release.append(i.text)\n",
    "            \n",
    "    r1 = s2.find_all('td', class_='ratingColumn imdbRating')\n",
    "    Rating = []\n",
    "    for i in r1:\n",
    "        for j in i.find_all('strong'):\n",
    "            Rating.append(j.text) \n",
    "            \n",
    "    imdb_Movies = pd.DataFrame()\n",
    "    imdb_Movies[\"Title\"] = Title[0:100]\n",
    "    imdb_Movies['Release'] = Release[0:100]\n",
    "    imdb_Movies['Rating'] = Rating[0:100]\n",
    "    \n",
    "    print(\"Shape of IMDB Movie Ratings is {} rows and {} columns \\n.\".format(*imdb_Movies.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameratingsyear=imdb_Movie(\"https://www.imdb.com/chart/top/\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "answer to 2nd question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd --- Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. Name, IMDB rating, Year of release) and save it in form of a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "nameratingsyears = pd.read_csv(\"nameratingsyear.csv\")\n",
    "print(nameratingsyears)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer to 3rd question "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4th --- Write a python program to scrap book name, author name, genre and book review of any 5 books from ‘www.bookpage.com’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page4=requests.get(\"https://bookpage.com/reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(page4.content,\"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn=soup.find_all(\"h4\",class_=\"italic\")\n",
    "\n",
    "Book_name=[]\n",
    "for i in bn:\n",
    "    for j in i.find_all(\"a\"):\n",
    "        Book_name.append(i.text.strip(\"\\n\"))\n",
    "print(Book_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an=soup.find_all(\"p\",class_=\"sans bold\")\n",
    "\n",
    "Author_name=[]\n",
    "for i in an:\n",
    "    Author_name.append(i.text.strip(\"\\n\"))\n",
    "    \n",
    "print(Author_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rn=soup.find_all(\"p\",class_=\"excerpt\")\n",
    "\n",
    "Review=[]\n",
    "for i in rn:\n",
    "    Review.append(i.text.strip(\"\\n\"))\n",
    "    \n",
    "print(Review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "Book_Reviews=pd.DataFrame()\n",
    "Book_Reviews[\"Book_name\"]=Book_name[0:5]\n",
    "Book_Reviews[\"Author_name\"]=Author_name[0:5]\n",
    "Book_Reviews[\"Review\"]=Review[0:5]\n",
    "\n",
    "print(\"shape of book review is {} rows and {} columns\".format(*Book_Reviews.shape))\n",
    "\n",
    "Book_Reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review(url):\n",
    "    p4 = requests.get(url)\n",
    "    \n",
    "    print(p4.status_code)\n",
    "    \n",
    "    s4 = BeautifulSoup(p4.content, \"html.parser\")\n",
    "    \n",
    "    bn = s4.find_all(\"h4\", class_=\"italic\")\n",
    "    Name = []\n",
    "    for i in bn:\n",
    "        for j in i.find_all(\"a\"):\n",
    "            Name.append(i.text.strip(\"\\n\"))\n",
    "            \n",
    "    an = s4.find_all(\"p\", class_='sans bold')\n",
    "    Author = []\n",
    "    for i in an:\n",
    "        Author.append(i.text.strip(\"\\n\"))\n",
    "    \n",
    "    rn = s4.find_all(\"p\", class_='excerpt')\n",
    "    Review = []\n",
    "    for i in rn:\n",
    "        Review.append(i.text.strip(\"\\n\"))\n",
    "        \n",
    "    Reviews = pd.DataFrame()\n",
    "    Reviews['Book_Name'] = Name[0:5]\n",
    "    Reviews['Author_Name'] = Author[0:5]\n",
    "    Reviews['Review'] = Review[0:5]\n",
    "    \n",
    "    print(\"Shape of Book Review is {} rows and {} columns.\".format(*Book_Reviews.shape))\n",
    "    print(Reviews.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review(\"Https://bookpage.com/reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer to 4th question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. You have to scrape:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ODI_Men(url):\n",
    "    p5 = requests.get(url)\n",
    "    p5.content\n",
    "    print(p5.status_code)\n",
    "    \n",
    "    s5 = BeautifulSoup(p5.content, \"html.parser\")\n",
    "    print(s5.prettify())\n",
    "    \n",
    "    t1 = s5.find_all('span', class_='u-hide-phablet')\n",
    "    Name = []\n",
    "    for i in t1:\n",
    "        Name.append(i.text)\n",
    "        \n",
    "    m1 = s5.find_all('td', class_='rankings-block__banner--matches')\n",
    "    Matches = []\n",
    "    for i in m1:\n",
    "        Matches.append(i.text)\n",
    "        \n",
    "    p1 = s5.find_all('td', class_='rankings-block__banner--points')\n",
    "    Point = []\n",
    "    for i in p1:\n",
    "        Point.append(i.text) \n",
    "    \n",
    "    r1 = s5.find_all('td', class_='rankings-block__banner--rating u-text-right')\n",
    "    Rating = []\n",
    "    for i in r1:\n",
    "        Rating.append(i.text.strip('\\n'))\n",
    "    Ratings = [] \n",
    "    Ratings.append(Rating[0].strip())\n",
    "    \n",
    "    m1 = s5.find_all('td', class_='table-body__cell u-center-text')\n",
    "    k = []\n",
    "    for i in range(0,len(m1),2):\n",
    "        k.append(m1[i])\n",
    "    for i in k:\n",
    "        Matches.append(i.get_text())\n",
    "        \n",
    "    k = []\n",
    "    for i in range(1,len(m1),2):\n",
    "        k.append(m1[i])\n",
    "    for i in k:\n",
    "        Point.append(i.get_text())\n",
    "        \n",
    "    r1 = s5.find_all('td', class_='table-body__cell u-text-right rating')\n",
    "    for i in r1:\n",
    "        Ratings.append(i.get_text().strip('\\n'))   \n",
    "   \n",
    "        \n",
    "    ODI_Men = pd.DataFrame()\n",
    "    ODI_Men['Team_Name'] = Name[0:10]\n",
    "    ODI_Men['Matches'] = Matches[0:10]\n",
    "    ODI_Men['Ratings'] = Ratings[0:10]\n",
    "    ODI_Men['Point'] = Point[0:10]\n",
    "    print(\"Shape of dataset is {} rows and {} columns\".format(*ODI_Men.shape))\n",
    "    print(ODI_Men.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ODI_Men(\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Top 10 ODI Batsmen in men along with the records of their team and rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ODI_Batsman(url):\n",
    "    p5 = requests.get(url)\n",
    "    p5.content\n",
    "    print(p5.status_code)\n",
    "    \n",
    "    s5 = BeautifulSoup(p5.content, \"html.parser\")\n",
    "    print(s5.prettify())\n",
    "    \n",
    "    p1 = s5.find_all('div', class_='rankings-block__banner--name-large')\n",
    "    Player = []\n",
    "    for i in p1:\n",
    "        Player.append(i.text)\n",
    "    \n",
    "    p2 = s5.find_all('td', class_='table-body__cell rankings-table__name name')\n",
    "    for i in p2:\n",
    "        for j in i.find_all(\"a\"):\n",
    "            Player.append(j.text)\n",
    "            \n",
    "    t1 = s5.find(\"div\", class_=\"rankings-block__banner--nationality\")\n",
    "    Team = []\n",
    "    Team.append(t1.text.strip())\n",
    "    \n",
    "    t2 = s5.find_all(\"span\", class_='table-body__logo-text')\n",
    "    for i in t2:\n",
    "        Team.append(i.text)\n",
    "        \n",
    "    r1 = s5.find_all('td', class_='table-body__cell rating')\n",
    "    Rating = []\n",
    "    for i in r1:\n",
    "        Rating.append(i.text)\n",
    "        \n",
    "    ODI_Batsman = pd.DataFrame()\n",
    "    ODI_Batsman['Team'] = Team[0:10]\n",
    "    ODI_Batsman['Player'] = Player[0:10]\n",
    "    ODI_Batsman['Rating'] = Rating[0:10]\n",
    "    print(\"Shape of dataset is {} rows and {} columns\".format(*ODI_Batsman.shape))\n",
    "    print(ODI_Batsman.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ODI_Batsman(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/Batting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ODI_Bowler(url):\n",
    "    p5 = requests.get(url)\n",
    "    p5.content\n",
    "    print(p5.status_code)\n",
    "    \n",
    "    s5 = BeautifulSoup(p5.content, \"html.parser\")\n",
    "    print(s5.prettify())\n",
    "    \n",
    "    p1 = s5.find_all('div', class_='rankings-block__banner--name-large')\n",
    "    Player = []\n",
    "    for i in p1:\n",
    "        Player.append(i.text)\n",
    "    \n",
    "    p2 = s5.find_all('td', class_='table-body__cell rankings-table__name name')\n",
    "    for i in p2:\n",
    "        for j in i.find_all(\"a\"):\n",
    "            Player.append(j.text)\n",
    "            \n",
    "    t1 = s5.find(\"div\", class_=\"rankings-block__banner--nationality\")\n",
    "    Team = []\n",
    "    Team.append(t1.text.strip())\n",
    "    \n",
    "    t2 = s5.find_all(\"span\", class_='table-body__logo-text')\n",
    "    for i in t2:\n",
    "        Team.append(i.text)\n",
    "        \n",
    "    r1 = s5.find_all('td', class_='table-body__cell rating')\n",
    "    Rating = []\n",
    "    for i in r1:\n",
    "        Rating.append(i.text)\n",
    "        \n",
    "    ODI_Bowler = pd.DataFrame()\n",
    "    ODI_Bowler['Team'] = Team[0:10]\n",
    "    ODI_Bowler['Player'] = Player[0:10]\n",
    "    ODI_Bowler['Rating'] = Rating[0:10]\n",
    "    print(\"Shape of dataset is {} rows and {} columns\".format(*ODI_Bowler.shape))\n",
    "    print(ODI_Bowler.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ODI_Batsman(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/Bowling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. You have to scrape:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ODI_Women(url):\n",
    "    p5 = requests.get(url)\n",
    "    p5.content\n",
    "    print(p5.status_code)\n",
    "    \n",
    "    s5 = BeautifulSoup(p5.content, \"html.parser\")\n",
    "    print(s5.prettify())\n",
    "    \n",
    "    t1 = s5.find_all('span', class_='u-hide-phablet')\n",
    "    Team_Name = []\n",
    "    for i in t1:\n",
    "        Team_Name.append(i.text)\n",
    "        \n",
    "    m1 = s5.find_all('td', class_='rankings-block__banner--matches')\n",
    "    Matches = []\n",
    "    for i in m1:\n",
    "        Matches.append(i.text)\n",
    "        \n",
    "    p1 = s5.find_all('td', class_='rankings-block__banner--points')\n",
    "    Point = []\n",
    "    for i in p1:\n",
    "        Point.append(i.text) \n",
    "    \n",
    "    r1 = s5.find_all('td', class_='rankings-block__banner--rating u-text-right')\n",
    "    Rating = []\n",
    "    for i in r1:\n",
    "        Rating.append(i.text.strip('\\n'))\n",
    "    Ratings = [] \n",
    "    Ratings.append(Rating[0].strip())\n",
    "    \n",
    "    m1 = s5.find_all('td', class_='table-body__cell u-center-text')\n",
    "    k = []\n",
    "    for i in range(0,len(m1),2):\n",
    "        k.append(m1[i])\n",
    "    for i in k:\n",
    "        Matches.append(i.get_text())\n",
    "        \n",
    "    k = []\n",
    "    for i in range(1,len(m1),2):\n",
    "        k.append(m1[i])\n",
    "    for i in k:\n",
    "        Point.append(i.get_text())\n",
    "        \n",
    "    r1 = s5.find_all('td', class_='table-body__cell u-text-right rating')\n",
    "    for i in r1:\n",
    "        Ratings.append(i.get_text().strip('\\n'))\n",
    "        \n",
    "    ODI_Women = pd.DataFrame()\n",
    "    ODI_Women['Team_Name'] = Team_Name[0:10]\n",
    "    ODI_Women['Matches'] = Matches[0:10]\n",
    "    ODI_Women['Ratings'] = Ratings[0:10]\n",
    "    ODI_Women['Point'] = Point[0:10]\n",
    "    print(\"Shape of dataset is {} rows and {} columns\".format(*ODI_Women.shape))\n",
    "    print(ODI_Women.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ODI_Women(\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Top 10 women’s ODI players along with the records of their team and rating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ODI_Women(url):\n",
    "    p5 = requests.get(url)\n",
    "    p5.content\n",
    "    print(p5.status_code)\n",
    "    \n",
    "    s5 = BeautifulSoup(p5.content, \"html.parser\")\n",
    "    print(s5.prettify())\n",
    "    \n",
    "    p1 = s5.find_all('div', class_='rankings-block__banner--name-large')\n",
    "    Player = []\n",
    "    for i in p1:\n",
    "        Player.append(i.text)\n",
    "    \n",
    "    p2 = s5.find_all('td', class_='table-body__cell rankings-table__name name')\n",
    "    for i in p2:\n",
    "        for j in i.find_all(\"a\"):\n",
    "            Player.append(j.text)\n",
    "            \n",
    "    t1 = s5.find(\"div\", class_=\"rankings-block__banner--nationality\")\n",
    "    Team = []\n",
    "    Team.append(t1.text.strip())\n",
    "    \n",
    "    t2 = s5.find_all(\"span\", class_='table-body__logo-text')\n",
    "    for i in t2:\n",
    "        Team.append(i.text)\n",
    "        \n",
    "    r1 = s5.find_all('td', class_='table-body__cell rating')\n",
    "    Rating = []\n",
    "    for i in r1:\n",
    "        Rating.append(i.text)\n",
    "        \n",
    "    ODI_Women = pd.DataFrame()\n",
    "    ODI_Women['Team'] = Team[0:10]\n",
    "    ODI_Women['Player'] = Player[0:10]\n",
    "    ODI_Women['Rating'] = Rating[0:10]\n",
    "    print(\"Shape of dataset is {} rows and {} columns\".format(*ODI_Women.shape))\n",
    "    print(ODI_Women.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ODI_Women(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ODI_Allrounder(url):\n",
    "    p5 = requests.get(url)\n",
    "    p5.content\n",
    "    print(p5.status_code)\n",
    "    \n",
    "    s5 = BeautifulSoup(p5.content, \"html.parser\")\n",
    "    print(s5.prettify())\n",
    "    \n",
    "    p1 = s5.find_all('div', class_='rankings-block__banner--name-large')\n",
    "    Player = []\n",
    "    for i in p1:\n",
    "        Player.append(i.text)\n",
    "    \n",
    "    p2 = s5.find_all('td', class_='table-body__cell rankings-table__name name')\n",
    "    for i in p2:\n",
    "        for j in i.find_all(\"a\"):\n",
    "            Player.append(j.text)\n",
    "            \n",
    "    t1 = s5.find(\"div\", class_=\"rankings-block__banner--nationality\")\n",
    "    Team = []\n",
    "    Team.append(t1.text.strip())\n",
    "    \n",
    "    t2 = s5.find_all(\"span\", class_='table-body__logo-text')\n",
    "    for i in t2:\n",
    "        Team.append(i.text)\n",
    "        \n",
    "    r1 = s5.find_all('td', class_='table-body__cell rating')\n",
    "    Rating = []\n",
    "    for i in r1:\n",
    "        Rating.append(i.text)\n",
    "        \n",
    "    ODI_Allrounder = pd.DataFrame()\n",
    "    ODI_Allrounder['Team'] = Team[0:10]\n",
    "    ODI_Allrounder['Player'] = Player[0:10]\n",
    "    ODI_Allrounder['Rating'] = Rating[0:10]\n",
    "    print(\"Shape of dataset is {} rows and {} columns\".format(*ODI_Allrounder.shape))\n",
    "    print(ODI_Allrounder.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ODI_Allrounder(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7----# Write a python program to scrape details of all the mobile phones under Rs. 20,000 listed on Amazon.in. The scraped data should include Product Name, Price, Image URL and Average Rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page7=requests.get(\"https://www.amazon.in/s?k=mobile+phones+under+20000&crid=1X4L8PH7F6SA9&sprefix=mobile+phones+under+%2Caps%2C681&ref=nb_sb_ss_ts-doa-p_9_20\")\n",
    "page7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Amazon(url):\n",
    "    p7 = requests.get(url)\n",
    "    p7.content\n",
    "    print(p7.status_code)\n",
    "    \n",
    "    s7 = BeautifulSoup(p7.content, 'html.parser')\n",
    "    print(s7.prettify())\n",
    "    \n",
    "    m_name = s7.find_all('span', class_='a-size-medium a-color-base a-text-normal')\n",
    "    m_price = s7.find_all('span', class_='a-price-whole')\n",
    "    m_rating = s7.find_all('span', class_=\"a-icon-alt\")\n",
    "    m_img = s7.find_all('img', class_=\"s-image\")\n",
    "    \n",
    "    Mobile = []\n",
    "    for i in m_name:\n",
    "        Mobile.append(i.get_text())\n",
    "        \n",
    "    Price = []\n",
    "    for i in m_price:\n",
    "        Price.append(i.get_text())\n",
    "        \n",
    "    Rating = []\n",
    "    for i in m_rating:\n",
    "        Rating.append(i.get_text())\n",
    "        \n",
    "    Image = []\n",
    "    for i in m_img:\n",
    "        Image.append(i.get('srcset'))\n",
    "        \n",
    "    Mobile_dataset = pd.DataFrame()\n",
    "    Mobile_dataset['Mobile_Name'] = Mobile[0:16]\n",
    "    Mobile_dataset['Mobile_Price'] = Price[0:16]\n",
    "    Mobile_dataset['Mobile_rating'] = Rating[0:16]\n",
    "    Mobile_dataset['Image_Link'] = Image[0:16]\n",
    "    print(\"Shape of dataset is {} rows and {} columns\".format(*Mobile_dataset.shape))\n",
    "    print(Mobile_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Amazon(\"https://www.amazon.in/s?k=mobile&rh=p_36%3A-20000000&qid=1621975129&rnid=1318502031&ref=sr_nr_p_36_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Write a python program to extract information about the local weather from the National Weather Service website of USA, https://www.weather.gov/ for the city, San Francisco. You need to extract data about 7 day extended forecast display for the city. The data should include period, short description, temperature and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page8=requests.get(\"https://forecast.weather.gov/MapClick.php?textField1=37.78&textField2=-122.42#.YLUHxagzZPY\")\n",
    "page8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(page4.content,\"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(page.content)\n",
    "soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weather(url):\n",
    "    p8 = requests.get(url)\n",
    "    print(p8.status_code)\n",
    "    \n",
    "    s8 = BeautifulSoup(p8.content, \"html.parser\")\n",
    "    \n",
    "    per = s8.find_all(\"p\", class_ =\"period-name\")\n",
    "    Day = []\n",
    "    for i in per:\n",
    "        Day.append(i.text)\n",
    "\n",
    "    te = s8.find_all('p', class_='temp temp-high')\n",
    "    temp = []\n",
    "    for i in te:\n",
    "        temp.append(i.text)\n",
    "        \n",
    "    descrip = s8.find_all(\"p\", class_=\"short-desc\")\n",
    "    Description = []\n",
    "    for i in descrip:\n",
    "        Description.append(i.text)\n",
    "        \n",
    "    Weather = pd.DataFrame()\n",
    "    Weather['Day'] = Day[0:5]\n",
    "    Weather['temp'] = temp[0:5]\n",
    "    Weather['Description'] = Description[0:5]\n",
    "    print(\"Shape of dataset is {} rows and {} columns\".format(*Weather.shape))\n",
    "    print(Weather.head(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weather(\"https://forecast.weather.gov/MapClick.php?textField1=37.78&textField2=-122.42#.YLUHxagzZPY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Write a python program to scrape fresher job listings from ‘https://internshala.com/’. It should include job title, company name, CTC, and apply date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page9=requests.get(\"https://internshala.com/fresher-jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page9.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(page9.content)\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles=soup.find(\"div\",class_=\"heading_4_5 profile\")\n",
    "job_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles=soup.find_all(\"div\",class_=\"heading_4_5 profile\")\n",
    "job_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "for i in job_titles:\n",
    "    job_title.append(i.text.replace(\"\\n\",\"\"))\n",
    "job_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles=soup.find(\"div\",class_=\"heading_4_5 profile\")\n",
    "job_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names=soup.find_all(\"a\",class_=\"link_display_like_text\")\n",
    "company_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name=[]\n",
    "for i in company_names:\n",
    "    company_name.append(i.text.strip())\n",
    "company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ctc=soup.find_all(\"div\",class_=\"item_body\")\n",
    "ctc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Internshala(url):\n",
    "    p9 = requests.get(url)\n",
    "    p9.content\n",
    "    print(p9.status_code)\n",
    "    \n",
    "    s9 = BeautifulSoup(p9.content, \"html.parser\")\n",
    "    print(s9.prettify())\n",
    "    \n",
    "    title = []\n",
    "    company = []\n",
    "    ctc = []\n",
    "    apply_date = []\n",
    "    \n",
    "    temp = []\n",
    "    \n",
    "    get_titles = s9.find_all('div',class_ = \"heading_4_5 profile\")   \n",
    "    get_companies = s9.find_all('a', class_ = 'link_display_like_text')\n",
    "    get_ctcadate = s9.find_all('div',class_ = \"item_body\", id = False)\n",
    "    \n",
    "    for cdate in get_ctcadate:\n",
    "        temp.append(cdate.text.strip())\n",
    "        \n",
    "    for t, c in zip(get_titles, get_companies):\n",
    "        title.append(t.text.strip())\n",
    "        company.append(c.text.strip())\n",
    "    \n",
    "    for i in temp:\n",
    "        ctc.append(i) if 'LPA' in i else apply_date.append(i)\n",
    "            \n",
    "    Internshala = pd.DataFrame({'Title': title,\n",
    "                     'Company': company,\n",
    "                     'CTC': ctc,\n",
    "                     'Apply Date': apply_date\n",
    "                     })\n",
    "    print(\"Shape of dataset is {} rows and {} columns\".format(*Internshala.shape))\n",
    "    print(Internshala.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Internshala(\"https://internshala.com/fresher-jobs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
